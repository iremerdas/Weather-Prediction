# -*- coding: utf-8 -*-
"""MLI_Essay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VSq-2FTQndbxG18LIRtQUZLsXNp3P0PG

**MAKİNE ÖĞRENMESİNE GİRİŞ DERSİ PROJE ÖDEVİ**

---

**Drive'a Bağlanma**
"""

from google.colab import drive
drive.mount('/content/drive')

"""

---

"""

import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt

"""# Veri Analizi

data1: 1 Ocak 2022 - 31 Aralık 2023 tarihleri arasındaki Sıcaklık, bağıl nem, rüzgar hızı  ve yönü, toplam bulut kapalılığı, güneşlenme süresi, basınç verilerini içerir.

data2: 1 Ocak 2024 - 31 Aralık 2024 tarihleri arasındaki Sıcaklık, bağıl nem, rüzgar hızı  ve yönü, toplam bulut kapalılığı, güneşlenme süresi, basınç verilerini içerir.

## Temel işlemler
"""

data1 = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/MLI/essay/data1.csv')
data2 = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/MLI/essay/data2.csv')

data1.head(10)

data2.head(10)

"""Her iki dosyada da veriler 9. satırda başlar. İlk 8 satır meta datadır ve silinebilir."""

data1_cleaned = data1.iloc[9:].reset_index(drop=True)
data1_cleaned

dataset2_cleaned = data2.iloc[9:].reset_index(drop=True)
dataset2_cleaned

print(data1_cleaned.describe())
print(dataset2_cleaned.describe())

print(data1_cleaned.dtypes)
print(dataset2_cleaned.dtypes)

print(data1_cleaned.isnull().sum())
print(dataset2_cleaned.isnull().sum())

"""Eksik değer kontrolü"""

print(data1_cleaned.isnull().mean())
print(dataset2_cleaned.isnull().mean())

"""Sütunların isminin değiştirilmesi"""

data1_cleaned.rename(columns={
    'location': 'Time',
    'Basel': 'Temperature',
    'Basel.1': 'Humidity',
    'Basel.2': 'Wind_Speed',
    'Basel.3': 'Wind_Direction',
    'Basel.4': 'Cloud_Cover',
    'Basel.5': 'Sunshine_Duration',
    'Basel.6': 'Pressure'
}, inplace=True)

dataset2_cleaned.rename(columns={
    'location': 'Time',
    'Basel': 'Temperature',
    'Basel.1': 'Humidity',
    'Basel.2': 'Wind_Speed',
    'Basel.3': 'Wind_Direction',
    'Basel.4': 'Cloud_Cover',
    'Basel.5': 'Sunshine_Duration',
    'Basel.6': 'Pressure'
}, inplace=True)

"""Ortalama ile eksik değerleri doldurma"""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
data1_cleaned[['Temperature', 'Humidity', 'Wind_Speed', 'Wind_Direction', 'Cloud_Cover', 'Sunshine_Duration', 'Pressure']] = imputer.fit_transform(data1_cleaned[['Temperature', 'Humidity', 'Wind_Speed', 'Wind_Direction', 'Cloud_Cover', 'Sunshine_Duration', 'Pressure']])

# Veri kaybı yaşandı mı diye kontrol etme
print(data1_cleaned.isnull().sum())
print(dataset2_cleaned.isnull().sum())

"""İki veri setini birleştirme"""

dataset = pd.concat([data1_cleaned, dataset2_cleaned], axis=0)

dataset

"""Birleştirme işlemi esnasında veri bozuldu mu? Eksik veri oluştu mu diye kontrol etme"""

dataset.isnull().sum()

"""## Analiz"""

# Sadece sayısal sütunları seçmek için tarih sütununu çıkarıyoruz
numeric_columns = dataset.drop(columns=dataset.columns[0])

corr_matrix = numeric_columns.corr()

plt.figure(figsize=(10, 6))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Korelasyon Matrisi")
plt.show()

"""Sıcaklık ile bağıl nem arasında güçlü bir negatif korelasyon var (-0.64). Bu, sıcaklık arttıkça bağıl nemin azaldığını gösterir.

Sıcaklık ile güneşlenme süresi arasında orta düzeyde pozitif bir ilişki var (0.49). Daha fazla güneş ışığı olduğunda sıcaklık artıyor.

Sıcaklık ile bulut kapalılığı arasında zayıf bir negatif ilişki var (-0.25). Bulutlar arttığında sıcaklık biraz düşüyor.

Sıcaklık ile basınç arasında zayıf bir negatif ilişki var (-0.25).

Sıcaklık ile rüzgar hızı ve yönü arasında anlamlı bir bağ yok (-0.04 ve 0.04).
"""

dataset = dataset.drop(columns=['Wind_Speed', 'Wind_Direction'])
dataset

# dataset i koplaya
df = dataset.copy()

"""---

**Zaman Serisi ile ilgili Temel İşlemler**
"""

# Zaman sütununu parçalama
df['Year'] = df['Time'].str[:4].astype(int)
df['Month'] = df['Time'].str[4:6].astype(int)
df['Day'] = df['Time'].str[6:8].astype(int)
df['Hour'] = df['Time'].str[9:11].astype(int)

# Time sütununu artık kullanmayacağız
df.drop('Time', axis=1, inplace=True)

# Mevsim sütunu eklemek
df['Season'] = df['Month'].apply(lambda x: 'Winter' if x in [12, 1 , 2] else
                                            'Spring' if x in [3, 4, 5] else
                                            'Summer' if x in [6, 7, 8] else 'Autumn')

# One-hot encoding ile mevsim sütununu dönüştürme
df = pd.get_dummies(df, columns=['Season'], drop_first=True)  # İlk sütunu düşürerek multicollinearity'i önlüyoruz

df

"""

---

"""

# Değişken dağılımlarını inceleme
for column in ['Temperature',	'Humidity',	'Cloud_Cover',	'Sunshine_Duration',	'Pressure']:
    plt.figure(figsize=(8, 4))
    sns.histplot(df[column], kde=True)
    plt.title(f"{column} Dağılımı")
    plt.show()

# Veri dağılımlarını inceleme
for column in ['Temperature',	'Humidity',	'Cloud_Cover',	'Sunshine_Duration',	'Pressure']:
    plt.figure(figsize=(8, 4))
    sns.boxplot(df[column])
    plt.title(f"{column} için Boxplot")
    plt.show()

df["Season_Autumn"] = 1 - (df["Season_Spring"] + df["Season_Summer"] + df["Season_Winter"])

df["Season"] = df[["Season_Spring", "Season_Summer", "Season_Winter", "Season_Autumn"]].idxmax(axis=1)

# İşlenecek sütunlar
columns_to_plot = ["Temperature", "Humidity", "Cloud_Cover", "Sunshine_Duration", "Pressure"]

# Her sütun için boxplot oluştur
for column in columns_to_plot:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x="Season", y=column, data=df)
    plt.title(f"Mevsimlere Göre {column} Dağılımı")
    plt.xlabel("Mevsim")
    plt.ylabel(column)
    plt.show()

"""Uç Değer Analizi yapma"""

def detect_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return (series < lower_bound) | (series > upper_bound)

seasons = ['Season_Spring', 'Season_Summer', 'Season_Winter', 'Season_Autumn']
columns_to_process = ['Temperature', 'Humidity', 'Cloud_Cover', 'Sunshine_Duration', 'Pressure']

max_iterations = 5  # Maksimum tekrar sayısı
for column in columns_to_process:
    for season in seasons:
        for iteration in range(max_iterations):
            # Mevsime ait maskeyi oluştur
            season_mask = df[season] == 1

            # Mevsim verilerini seç
            season_data = df.loc[season_mask, column]

            # Uç değerleri tespit et
            outliers = detect_outliers(season_data)

            # Uç değer yoksa dur
            if outliers.sum() == 0:
                break

            # Mevsim ortalamasını hesapla (uç değerler hariç)
            season_mean = season_data[~outliers].mean()

            # Uç değerleri mevsim ortalamasıyla değiştir
            df.loc[season_mask & outliers, column] = season_mean

df["Season"] = df[["Season_Spring", "Season_Summer", "Season_Winter", "Season_Autumn"]].idxmax(axis=1)

# İşlenecek sütunlar
columns_to_plot = ["Temperature", "Humidity", "Cloud_Cover", "Sunshine_Duration", "Pressure"]

# Her sütun için boxplot oluştur
for column in columns_to_plot:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x="Season", y=column, data=df)
    plt.title(f"Mevsimlere Göre {column} Dağılımı")
    plt.xlabel("Mevsim")
    plt.ylabel(column)
    plt.show()

"""Standartlaştırma"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

# Her mevsim için standartlaştırma işlemi
for season in seasons:
    # Sezon verilerini seç
    season_data = df[df[season] == 1]

    # Sadece standartlaştırılacak kolonları al
    season_columns = season_data[columns_to_process]

    # Standartlaştırma işlemi
    standardized_data = scaler.fit_transform(season_columns)

    # Standartlaştırılmış veriyi geri df'ye yerleştir
    df.loc[df[season] == 1, columns_to_process] = standardized_data

df = df.drop(columns=['Season_Autumn'])

df = df.drop(columns=['Season'])

df

"""---

## Döngüsel Özellikler Ekleme
"""

# Döngüsel özellikler
df['Hour_Sin'] = np.sin(2 * np.pi * df['Hour'] / 24)
df['Hour_Cos'] = np.cos(2 * np.pi * df['Hour'] / 24)
df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)
df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)

print(df)

"""Geçmiş sıcaklık değerlerini lag özellikleri olarak ekle"""

df = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/mli/dataset4.csv')

df['Day_Sin'] = np.sin(2 * np.pi * df['Day'] / 7)
df['Day_Cos'] = np.cos(2 * np.pi * df['Day'] / 7)

# Lag özellikleri
df['Temperature_Lag1'] = df['Temperature'].shift(1)
df['Temperature_Lag2'] = df['Temperature'].shift(2)
df['Temperature_Lag3'] = df['Temperature'].shift(3)

# İlk birkaç satır NaN olacak
df.dropna(inplace=True)

print(df)

"""Geçmiş nem değerlerini lag özellikleri olarak ekle"""

# Lag özellikleri
df['Humidity_Lag1'] = df['Humidity'].shift(1)
df['Humidity_Lag2'] = df['Humidity'].shift(2)
df['Humidity_Lag3'] = df['Humidity'].shift(3)

# İlk birkaç satır NaN olacak
df.dropna(inplace=True)

print(df)

"""Geçmiş bulut kapalılığı değerlerini lag özellikleri olarak ekle"""

# Lag özellikleri
df['Cloud_Cover_Lag1'] = df['Cloud_Cover'].shift(1)
df['Cloud_Cover_Lag2'] = df['Cloud_Cover'].shift(2)
df['Cloud_Cover_Lag3'] = df['Cloud_Cover'].shift(3)

# İlk birkaç satır NaN olacak
df.dropna(inplace=True)

print(df)

"""Geçmiş güneşlenme süresi değerlerini lag özellikleri olarak ekle"""

# Lag özellikleri
df['Sunshine_Duration_Lag1'] = df['Sunshine_Duration'].shift(1)
df['Sunshine_Duration_Lag2'] = df['Sunshine_Duration'].shift(2)
df['Sunshine_Duration_Lag3'] = df['Sunshine_Duration'].shift(3)

# İlk birkaç satır NaN olacak
df.dropna(inplace=True)

print(df)

"""Geçmiş basınç değerlerini lag özellikleri olarak ekle"""

# Lag özellikleri
df['Pressure_Lag1'] = df['Pressure'].shift(1)
df['Pressure_Lag2'] = df['Pressure'].shift(2)
df['Pressure_Lag3'] = df['Pressure'].shift(3)

# İlk birkaç satır NaN olacak
df.dropna(inplace=True)

print(df)

df.isnull().sum()

"""Geçmiş sıcaklık verilerine dayalı istatistiksel özellikler ekle"""

# Son 24 saatin ortalaması, maksimumu ve minimumu
df['Last24_Mean'] = df['Temperature'].rolling(window=24).mean()
df['Last24_Max'] = df['Temperature'].rolling(window=24).max()
df['Last24_Min'] = df['Temperature'].rolling(window=24).min()

print(df)

"""Geçmiş nem verilerine dayalı istatistiksel özellikler ekle"""

# Son 24 saatin ortalaması, maksimumu ve minimumu
df['Last24Humidity_Mean'] = df['Humidity'].rolling(window=24).mean()
df['Last24Humidity_Max'] = df['Humidity'].rolling(window=24).max()
df['Last24Humidity_Min'] = df['Humidity'].rolling(window=24).min()

print(df)

# Eksik değerler oluştu, bunları dolduruyoruz
df.fillna(method='bfill', inplace=True)

df.isnull().sum()

"""Geçmiş bulut kapalılığı verilerine dayalı istatistiksel özellikler ekle"""

# Son 24 saatin ortalaması, maksimumu ve minimumu
df['Last24Cloud_Cover_Mean'] = df['Cloud_Cover'].rolling(window=24).mean()
df['Last24Cloud_Cover_Max'] = df['Cloud_Cover'].rolling(window=24).max()
df['Last24Cloud_Cover_Min'] = df['Cloud_Cover'].rolling(window=24).min()

print(df)

# Eksik değerler oluştu, bunları dolduruyoruz
df.fillna(method='bfill', inplace=True)

"""Geçmiş güneşlenme süresi verilerine dayalı istatistiksel özellikler ekle"""

# Son 24 saatin ortalaması, maksimumu ve minimumu
df['Last24Sunshine_Duration_Mean'] = df['Sunshine_Duration'].rolling(window=24).mean()
df['Last24Sunshine_Duration_Max'] = df['Sunshine_Duration'].rolling(window=24).max()
df['Last24Sunshine_Duration_Min'] = df['Sunshine_Duration'].rolling(window=24).min()

print(df)

# Eksik değerler oluştu, bunları dolduruyoruz
df.fillna(method='bfill', inplace=True)

"""Geçmiş basınç verilerine dayalı istatistiksel özellikler ekle"""

# Son 24 saatin ortalaması, maksimumu ve minimumu
df['Last24Pressure_Mean'] = df['Pressure'].rolling(window=24).mean()
df['Last24Pressure_Max'] = df['Pressure'].rolling(window=24).max()
df['Last24Pressure_Min'] = df['Pressure'].rolling(window=24).min()

print(df)

# Eksik değerler oluştu, bunları dolduruyoruz
df.fillna(method='bfill', inplace=True)

df.isnull().sum()

# Eksik değerler oluştu, bunları dolduruyoruz
df.fillna(method='bfill', inplace=True)

"""dataset1: döngüsel özl (saat, ay)

dataset2: döngüsel özl (saat, ay) + uç değer analizi + standartlaştırma

dataset2: döngüsel özl (saat, ay, gün) + uç değer analizi + standartlaştırma

"""

df

df = df.drop(columns=['Hour', 'Month', 'Day'])
df

"""---

# Model
"""

df = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/MLI/essay/dataset4.csv')

df

data = pd.DataFrame(df)

"""## Veri Seti Ayırma

Random Bölme
"""

from sklearn.model_selection import train_test_split

# Özellik ve hedef değişkenleri ayırma
X = df[['Day_Sin', 'Day_Cos', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Hour_Sin', 'Hour_Cos', 'Month_Sin', 'Month_Cos',
        'Temperature_Lag1', 'Temperature_Lag2', 'Temperature_Lag3', 'Last24_Mean', 'Last24_Max', 'Last24_Min']]
y = df[['Temperature', 'Humidity', 'Cloud_Cover', 'Sunshine_Duration', 'Pressure']]

# Veri setini eğitim ve test olarak ayırma
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(X_train.head(), y_train.head())

"""Zaman Tabanlı Bölme"""

X = df[['Day_Sin', 'Day_Cos', 'Season_Spring', 'Season_Summer', 'Season_Winter', 'Hour_Sin', 'Hour_Cos', 'Month_Sin', 'Month_Cos',
        'Temperature_Lag1', 'Temperature_Lag2', 'Temperature_Lag3', 'Last24_Mean', 'Last24_Max', 'Last24_Min']]
y = df[['Temperature', 'Humidity', 'Cloud_Cover', 'Sunshine_Duration', 'Pressure']]

df = df.sort_values(by='Year')

train_size = int(len(df) * 0.8)

# Eğitim ve test setlerini ayırın
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

"""Zaman Serisi Çapraz Doğrulama"""

from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)  # 5 katlı çapraz doğrulama
for train_index, test_index in tscv.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

"""## Model oluşturma"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.model_selection import GridSearchCV
from sklearn.multioutput import MultiOutputRegressor

from sklearn.ensemble import RandomForestRegressor

param_grid = {
    'estimator__n_estimators': [150, 200, 300, 400],
    'estimator__max_depth': [None, 10, 20, 30],
    'estimator__min_samples_split': [2, 5, 10],
    'estimator__min_samples_leaf': [1, 2, 4]
}

# MultiOutputRegressor ile GridSearchCV
base_model = RandomForestRegressor(random_state=42)
multi_output_regressor = MultiOutputRegressor(base_model)

grid_rfg= GridSearchCV(multi_output_regressor, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)
grid_rfg.fit(X_train, y_train)

print("En iyi hiperparametreler:", grid_rfg.best_params_)

y_pred_rgf = grid_rfg.best_estimator_.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred_rgf))
print("MAE:", mean_absolute_error(y_test, y_pred_rgf))
print("R2 Score:", r2_score(y_test, y_pred_rgf))

"""Deneme 1:

*   En iyi hiperparametreler: {'estimator__max_depth': None, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__n_estimators': 150}
*   Sonuçlar: {MSE: 72.45045335961979, MAE: 4.3277727108241875, R2 Score: 0.8994304258000858}


"""

from lightgbm import LGBMRegressor

param_grid_lgb = {
    'estimator__n_estimators': [100, 200, 500],
    'estimator__learning_rate': [0.01, 0.05, 0.1],
    'estimator__max_depth': [3, 5, 10],
    'estimator__subsample': [0.6, 0.8, 1.0],
    'estimator__colsample_bytree': [0.6, 0.8, 1.0]
}

lgb_model = MultiOutputRegressor(LGBMRegressor())
grid_lgb = GridSearchCV(estimator=lgb_model, param_grid=param_grid_lgb, cv=3, scoring='neg_mean_squared_error', verbose=1)
grid_lgb.fit(X_train, y_train)


print("LightGBM Best Params:", grid_lgb.best_params_)
print("LightGBM Best Score:", -grid_lgb.best_score_)

y_pred_lgb = grid_lgb.best_estimator_.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred_lgb))
print("MAE:", mean_absolute_error(y_test, y_pred_lgb))
print("R2 Score:", r2_score(y_test, y_pred_lgb))

"""SVR"""

from sklearn.svm import SVR
from sklearn.multioutput import MultiOutputRegressor

param_grid_svr = {
    'estimator__kernel': ['linear', 'rbf', 'poly'],
    'estimator__C': [0.1, 1, 10, 100],
    'estimator__gamma': ['scale', 'auto']
}

svr_model = MultiOutputRegressor(SVR())
grid_svr = GridSearchCV(estimator=svr_model, param_grid=param_grid_svr, cv=3, scoring='neg_mean_squared_error', verbose=1)
grid_svr.fit(X_train, y_train)

print("SVR Best Params:", grid_svr.best_params_)
print("SVR Best Score:", -grid_svr.best_score_)

y_pred_svr = grid_svr.best_estimator_.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred_svr))
print("MAE:", mean_absolute_error(y_test, y_pred_svr))
print("R2 Score:", r2_score(y_test, y_pred_svr))

from sklearn.multioutput import RegressorChain
from sklearn.svm import SVR
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

param_grid = {
    'base_estimator__C': [0.1, 1, 10],          # C parametresi (regülasyon)
    'base_estimator__epsilon': [0.1, 0.2, 0.3], # Hata toleransı
    'base_estimator__kernel': ['rbf', 'linear'] # Çekirdek fonksiyonları
}

base_svr = SVR()

regressor_chain = RegressorChain(base_estimator=base_svr, order='random', random_state=42)

grid_search = GridSearchCV(estimator=regressor_chain, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
print("En İyi Parametreler:", best_params)

y_pred = best_model.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred))
print("R2 Score:", r2_score(y_test, y_pred))
print("MAE:", mean_absolute_error(y_test, y_pred_svr))

"""

---

"""

from sklearn.multioutput import RegressorChain
from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import GridSearchCV
from sklearn.multioutput import MultiOutputRegressor
from sklearn.ensemble import RandomForestRegressor

param_grid = {
    'base_estimator__learning_rate': [0.01, 0.05, 0.1],  # Öğrenme hızı
    'base_estimator__n_estimators': [100, 200, 500],     # Ağaç sayısı
    'base_estimator__max_depth': [3, 5, 7],              # Maksimum derinlik
    'base_estimator__num_leaves': [20, 31, 40],          # Yaprak sayısı
    'base_estimator__min_child_samples': [10, 20, 30]    # Minimum yaprak örnek sayısı
}

base_lgbm = LGBMRegressor(random_state=42)

regressor_chain = RegressorChain(base_estimator=base_lgbm, order='random', random_state=42)

grid_search = GridSearchCV(estimator=regressor_chain, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
print("En İyi Parametreler:", best_params)

y_pred_lgb = best_model.predict(X_test)

# genel
mse = mean_squared_error(y_test, y_pred_lgb)
r2 = r2_score(y_test, y_pred_lgb)
mae = mean_absolute_error(y_test, y_pred_lgb)

print(f"Genel MSE: {mse:.4f}")
print(f"Genel R2 Score: {r2:.4f}")
print(f"Genel MAE: {mae:.4f}")

for i, column in enumerate(y.columns):
    mse_i = mean_squared_error(y_test.iloc[:, i], y_pred_lgb[:, i])
    r2_i = r2_score(y_test.iloc[:, i], y_pred_lgb[:, i])
    print(f"\nHedef Değişken: {column}")
    print(f"MSE: {mse_i:.4f}")
    print(f"R2 Score: {r2_i:.4f}")
    print(f"MAE Score: {mae_i:.4f}")

"""En İyi Parametreler: {'base_estimator__learning_rate': 0.05, 'base_estimator__max_depth': 7, 'base_estimator__min_child_samples': 30, 'base_estimator__n_estimators': 200, 'base_estimator__num_leaves': 40}


Genel MSE: 0.1012
Genel R2 Score: 0.9019

Hedef Değişken: Temperature
MSE: 0.0475
R2 Score: 0.9555

Hedef Değişken: Humidity
MSE: 0.1002
R2 Score: 0.8901

Hedef Değişken: Cloud_Cover
MSE: 0.1608
R2 Score: 0.8485

Hedef Değişken: Sunshine_Duration
MSE: 0.1763
R2 Score: 0.8381

Hedef Değişken: Pressure
MSE: 0.0211
R2 Score: 0.9774
"""

from sklearn.svm import SVR
param_grid = {
    'base_estimator__C': [0.1, 1, 10],          # C parametresi (regülasyon)
    'base_estimator__epsilon': [0.1, 0.2, 0.3], # Hata toleransı
    'base_estimator__kernel': ['rbf', 'linear'] # Çekirdek fonksiyonları
}

base_svr = SVR()

regressor_chain = RegressorChain(base_estimator=base_svr, order='random', random_state=42)

grid_search = GridSearchCV(estimator=regressor_chain, param_grid=param_grid, cv=3, scoring='r2', n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
print("En İyi Parametreler:", best_params)

y_pred_svr = best_model.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred_svr))
print("R2 Score:", r2_score(y_test, y_pred_svr))
print("MAE:", mean_absolute_error(y_test,y_pred_svr))

for i, column in enumerate(y.columns):
    mse = mean_squared_error(y_test.iloc[:, i], y_pred_svr[:, i])
    r2 = r2_score(y_test.iloc[:, i], y_pred_svr[:, i])
    mae = mean_absolute_error(y_test.iloc[:, i], y_pred_svr[:, i])
    print(f"Hedef Değişken: {column}")
    print(f"MSE: {mse:.4f}")
    print(f"R2 Score: {r2:.4f}")
    print(f"MAE Score: {mae:.4f}")

"""En İyi Parametreler: {'base_estimator__C': 0.1, 'base_estimator__epsilon': 0.3, 'base_estimator__kernel': 'linear'}

MSE: 0.15297875513535755
R2 Score: 0.8506708062641716
MAE: 0.22252414176455212

Hedef Değişken: Temperature
MSE: 0.0838
R2 Score: 0.9216

Hedef Değişken: Humidity
MSE: 0.1878
R2 Score: 0.7940

Hedef Değişken: Cloud_Cover
MSE: 0.1784
R2 Score: 0.8319

Hedef Değişken: Sunshine_Duration
MSE: 0.2823
R2 Score: 0.7407

Hedef Değişken: Pressure
MSE: 0.0326
R2 Score: 0.9651
"""

base_model = RandomForestRegressor(random_state=42)

chain_model = RegressorChain(base_model)

param_grid = {
    'base_estimator__n_estimators': [500, 600, 700],
    'base_estimator__max_depth': [10, 20, None],
    'base_estimator__min_samples_split': [5, 10, 15],
    'base_estimator__min_samples_leaf': [2, 4, 6]
}

grid_search = GridSearchCV(estimator=chain_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# genel
print(f"Best Parameters: {grid_search.best_params_}")

"""
Best Parameters: {'base_estimator__max_depth': 10, 'base_estimator__min_samples_leaf': 4, 'base_estimator__min_samples_split': 10, 'base_estimator__n_estimators': 500}
"""

y_pred_rgfchain = grid_search.predict(X_test)
print("MSE:", mean_squared_error(y_test, y_pred_rgfchain))
print("MAE:", mean_absolute_error(y_test, y_pred_rgfchain))
print("R2 Score:", r2_score(y_test, y_pred_rgfchain))

for i, column in enumerate(y.columns):
    mse = mean_squared_error(y_test.iloc[:, i], y_pred_rgfchain[:, i])
    r2 = r2_score(y_test.iloc[:, i], y_pred_rgfchain[:, i])
    mae = mean_absolute_error(y_test.iloc[:, i], y_pred_rgfchain[:, i])
    print(f"Hedef Değişken: {column}")
    print(f"MSE: {mse:.4f}")
    print(f"R2 Score: {r2:.4f}")
    print(f"MAE Score: {mae:.4f}")

def generate_features_with_history(date_str, historical_data):
    import numpy as np
    import pandas as pd

    date = datetime.strptime(date_str, "%Y-%m-%d")
    day_of_year = date.timetuple().tm_yday

    day_sin = np.sin(2 * np.pi * day_of_year / 365)
    day_cos = np.cos(2 * np.pi * day_of_year / 365)

    hours = np.arange(24)
    hour_sin = np.sin(2 * np.pi * hours / 24)
    hour_cos = np.cos(2 * np.pi * hours / 24)

    month = date.month
    season_spring = 3 <= month <= 5
    season_summer = 6 <= month <= 8
    season_winter = month == 12 or 1 <= month <= 2

    historical_data = historical_data.sort_values(by='Date')
    last_day_data = historical_data[historical_data['Date'] < date_str].tail(24)

    if last_day_data.empty:
        raise ValueError("Geçmiş veriler tahmin için yetersiz!")

    last24_mean = last_day_data['Temperature'].mean()
    last24_max = last_day_data['Temperature'].max()
    last24_min = last_day_data['Temperature'].min()
    lag1 = last_day_data['Temperature'].iloc[-1]
    lag2 = last_day_data['Temperature'].iloc[-2] if len(last_day_data) > 1 else lag1
    lag3 = last_day_data['Temperature'].iloc[-3] if len(last_day_data) > 2 else lag2

    features = pd.DataFrame({
        'Hour_Sin': hour_sin,
        'Hour_Cos': hour_cos,
        'Day_Sin': [day_sin] * 24,
        'Day_Cos': [day_cos] * 24,
        'Season_Spring': [season_spring] * 24,
        'Season_Summer': [season_summer] * 24,
        'Season_Winter': [season_winter] * 24,
        'Last24_Mean': [last24_mean] * 24,
        'Last24_Max': [last24_max] * 24,
        'Last24_Min': [last24_min] * 24,
        'Temperature_Lag1': [lag1] * 24,
        'Temperature_Lag2': [lag2] * 24,
        'Temperature_Lag3': [lag3] * 24,
    })
    return features

def predict_for_date_with_history(model, date_str, historical_data):
    features = generate_features_with_history(date_str, historical_data)
    predictions = model.predict(features)
    predictions_df = pd.DataFrame(predictions, columns=['Temperature', 'Humidity', 'Cloud_Cover', 'Sunshine_Duration', 'Pressure'])
    predictions_df['Hour'] = np.arange(24)
    return predictions_df

historical_data = pd.DataFrame({
    'Date': pd.date_range(start="2025-01-01", periods=300, freq="H"),
    'Temperature': np.random.normal(15, 5, 300),
    'Humidity': np.random.uniform(30, 90, 300),
    'Cloud_Cover': np.random.uniform(0, 100, 300),
    'Sunshine_Duration': np.random.uniform(0, 10, 300),
    'Pressure': np.random.uniform(950, 1050, 300),
})

# Kullanıcı girişi ile tahmin:
date_input = "2023-01-20"
predicted_weather = predict_for_date_with_history(chain_model, date_input, historical_data)
print(predicted_weather)